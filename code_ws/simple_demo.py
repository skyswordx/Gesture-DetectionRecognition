"""
ç®€åŒ–ç‰ˆäººä½“æ£€æµ‹æ¼”ç¤º
å¿«é€Ÿæµ‹è¯•MediaPipeåŠŸèƒ½
"""

import cv2
import mediapipe as mp
import numpy as np

class SimpleHumanDetection:
    def __init__(self):
        # MediaPipeåˆå§‹åŒ–
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            smooth_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        self.gesture_history = []
        
    def detect_human_bbox(self, image, landmarks):
        """æ£€æµ‹äººä½“è¾¹ç•Œæ¡†"""
        if not landmarks:
            return None
        
        h, w = image.shape[:2]
        x_coords = [landmark.x * w for landmark in landmarks.landmark]
        y_coords = [landmark.y * h for landmark in landmarks.landmark]
        
        x_min = int(max(0, min(x_coords) - 20))
        y_min = int(max(0, min(y_coords) - 20))
        x_max = int(min(w, max(x_coords) + 20))
        y_max = int(min(h, max(y_coords) + 20))
        
        return (x_min, y_min, x_max, y_max)
    
    def recognize_simple_gesture(self, landmarks):
        """ç®€å•å§¿åŠ¿è¯†åˆ«"""
        if not landmarks:
            return "æœªæ£€æµ‹åˆ°äººä½“"
        
        try:
            # è·å–å…³é”®ç‚¹
            left_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_WRIST]
            right_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_WRIST]
            left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]
            right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]
            nose = landmarks.landmark[self.mp_pose.PoseLandmark.NOSE]
            
            # ç®€å•çš„å§¿åŠ¿åˆ¤æ–­
            if left_wrist.y < left_shoulder.y and right_wrist.y < right_shoulder.y:
                if abs(left_wrist.y - right_wrist.y) < 0.1:
                    return "åŒæ‰‹ä¸¾èµ· ğŸ™Œ"
                else:
                    return "ä¸€åªæ‰‹ä¸¾èµ· âœ‹"
            elif left_wrist.y < left_shoulder.y:
                return "å·¦æ‰‹ä¸¾èµ· âœ‹"
            elif right_wrist.y < right_shoulder.y:
                return "å³æ‰‹ä¸¾èµ· âœ‹"
            elif (abs(left_wrist.x - left_shoulder.x) > 0.2 and 
                  abs(right_wrist.x - right_shoulder.x) > 0.2):
                return "åŒè‡‚å±•å¼€ ğŸ¤¸"
            else:
                return "æ­£å¸¸ç«™ç«‹ ğŸ§"
                
        except Exception as e:
            return "è¯†åˆ«é”™è¯¯"
    
    def run(self):
        """è¿è¡Œæ£€æµ‹ç³»ç»Ÿ"""
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´!")
            print("è¯·æ£€æŸ¥:")
            print("1. æ‘„åƒå¤´æ˜¯å¦è¢«å…¶ä»–ç¨‹åºå ç”¨")
            print("2. ç³»ç»Ÿæ˜¯å¦æˆäºˆäº†æ‘„åƒå¤´æƒé™")
            return
        
        print("äººä½“æ£€æµ‹ç³»ç»Ÿå·²å¯åŠ¨!")
        print("æŒ‰é”®è¯´æ˜:")
        print("  'q' - é€€å‡ºç¨‹åº")
        print("  's' - æˆªå›¾ä¿å­˜")
        print("  ç©ºæ ¼ - æš‚åœ/ç»§ç»­")
        
        paused = False
        frame_count = 0
        
        while True:
            if not paused:
                ret, frame = cap.read()
                if not ret:
                    print("æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                    break
                
                # é•œåƒç¿»è½¬
                frame = cv2.flip(frame, 1)
                
                # è½¬æ¢é¢œè‰²ç©ºé—´
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.pose.process(rgb_frame)
                
                # ç»˜åˆ¶ç»“æœ
                if results.pose_landmarks:
                    # ç»˜åˆ¶éª¨éª¼ç‚¹å’Œè¿æ¥çº¿
                    self.mp_drawing.draw_landmarks(
                        frame, 
                        results.pose_landmarks, 
                        self.mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()
                    )
                    
                    # ç»˜åˆ¶äººä½“è¾¹ç•Œæ¡†
                    bbox = self.detect_human_bbox(frame, results.pose_landmarks)
                    if bbox:
                        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
                        cv2.putText(frame, "Human Detected", (bbox[0], bbox[1]-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    
                    # å§¿åŠ¿è¯†åˆ«
                    gesture = self.recognize_simple_gesture(results.pose_landmarks)
                    cv2.putText(frame, f"Gesture: {gesture}", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
                    
                    # æ˜¾ç¤ºå…³é”®ç‚¹æ•°é‡
                    landmark_count = len(results.pose_landmarks.landmark)
                    cv2.putText(frame, f"Landmarks: {landmark_count}", (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
                
                else:
                    cv2.putText(frame, "No Human Detected", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
                
                # æ˜¾ç¤ºå¸§æ•°
                cv2.putText(frame, f"Frame: {frame_count}", (10, frame.shape[0]-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                frame_count += 1
            
            # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
            status = "PAUSED" if paused else "RUNNING"
            cv2.putText(frame, status, (frame.shape[1]-100, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # æ˜¾ç¤ºç”»é¢
            cv2.imshow('Human Detection & Pose Recognition', frame)
            
            # æŒ‰é”®å¤„ç†
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord(' '):
                paused = not paused
                print(f"{'æš‚åœ' if paused else 'ç»§ç»­'}æ£€æµ‹")
            elif key == ord('s'):
                filename = f"screenshot_{frame_count}.jpg"
                cv2.imwrite(filename, frame)
                print(f"æˆªå›¾å·²ä¿å­˜: {filename}")
        
        # æ¸…ç†èµ„æº
        cap.release()
        cv2.destroyAllWindows()
        print("ç³»ç»Ÿå·²é€€å‡º")

def main():
    """ä¸»å‡½æ•°"""
    print("=== MediaPipe äººä½“æ£€æµ‹ä¸å§¿åŠ¿è¯†åˆ«ç³»ç»Ÿ ===")
    print("åŠŸèƒ½:")
    print("1. äººå½¢æ¡†å®šæ£€æµ‹")
    print("2. éª¨éª¼å…³é”®ç‚¹è¯†åˆ«") 
    print("3. åŸºç¡€å§¿åŠ¿è¯†åˆ«")
    print("=" * 40)
    
    try:
        detector = SimpleHumanDetection()
        detector.run()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"ç¨‹åºé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()
